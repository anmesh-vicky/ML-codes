{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This particular assignment focuses on text classification using CNN. It has been picking up pace over the past few years. So, I thought this would be a good exercise to try out. The dataset is provided to you and there will be specific instrucions on how to curate the data, split into train and validation and the like.  You will be using MXnet for this task.  The data comprises tweets pertaining to common causes of cancer. The objective is to classify the tweets as medically relevant or not.  The dataset is skewed with positive class or 'yes' being 6 times less frequent than the negative class or 'no'. (Total marks = 50). Individual marks to the sub-problems are given in bracket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 1298)\n"
     ]
    }
   ],
   "source": [
    "#Anmesh Choudhury\n",
    "#16NA30003\n",
    "# these are the modules you are allowed to work with. \n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import itertools\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import sys, os\n",
    "import re\n",
    "import random\n",
    "from collections import Counter\n",
    "from io import open\n",
    "from sklearn.metrics import f1_score\n",
    "'''\n",
    "First job is to clean and preprocess the social media text. (5)\n",
    "\n",
    "1) Replace URLs and mentions (i.e strings which are preceeded with @)\n",
    "2) Segment #hastags \n",
    "3) Remove emoticons and other unicode characters\n",
    "'''\n",
    "\n",
    "def preprocess_tweet(input_text):\n",
    "    '''\n",
    "    Input: The input string read directly from the file\n",
    "    \n",
    "    Output: Pre-processed tweet text\n",
    "    '''\n",
    "    cleaned_text = re.sub(r\"http\\S+\", \"\", input_text)\n",
    "    cleaned_text = re.sub(r'(\\s)@\\w+', r'\\1', cleaned_text)\n",
    "    cleaned_text = (cleaned_text.encode('ascii', 'ignore')).decode(\"utf-8\")\n",
    "    cleaned_text = cleaned_text.replace(\"#\", \"\")\n",
    "    cleaned_text = cleaned_text.replace(\"@\", \"\")\n",
    "    cleaned_text = re.sub(r'((?<=[a-z])[A-Z]|(?<!\\A)[A-Z](?=[a-z]))', r' \\1', cleaned_text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "# read the input file and create the set of positive examples and negative examples. \n",
    "\n",
    "file=open('cancer_data.tsv',encoding=\"utf8\")\n",
    "pos_data=[]\n",
    "neg_data=[]\n",
    "\n",
    "for line in file:\n",
    "    line=line.strip().split('\\t')\n",
    "    text2= preprocess_tweet(line[0]).strip().split()\n",
    "    if line[1]=='yes':\n",
    "        pos_data.append(text2)\n",
    "    if line[1]=='no':\n",
    "        neg_data.append(text2)\n",
    "\n",
    "print(len(pos_data), len(neg_data))     \n",
    "\n",
    "sentences= list(pos_data)\n",
    "sentences.extend(neg_data)\n",
    "pos_labels= [1 for _ in pos_data]\n",
    "neg_labels= [0 for _ in neg_data]\n",
    "y=list(pos_labels)\n",
    "y.extend(neg_labels)\n",
    "y=np.array(y)\n",
    "\n",
    "'''\n",
    "After this you will obtain the following :\n",
    "\n",
    "1) sentences =  List of sentences having the positive and negative examples with all the positive examples first\n",
    "2) y = List of labels with the positive labels first.\n",
    "'''\n",
    "\n",
    "'''\n",
    "Before running the CNN there are a few things one needs to take care of: (5)\n",
    "\n",
    "1) Pad the sentences so that all of them are of the same length\n",
    "2) Build a vocabulary comprising all unique words that occur in the corpus\n",
    "3) Convert each sentence into a corresponding vector by replacing each word in the sentence with the index in the vocabulary. \n",
    "\n",
    "Example :\n",
    "S1 = a b a c\n",
    "S2 = d c a \n",
    "\n",
    "Step 1:  S1= a b a c, \n",
    "         S2 =d c a </s> \n",
    "         (Both sentences are of equal length). \n",
    "\n",
    "Step 2:  voc={a:1, b:2, c:3, d:4, </s>: 5}\n",
    "\n",
    "Step 3:  S1= [1,2,1,3]\n",
    "         S2= [4,3,1,5]\n",
    "\n",
    "'''\n",
    "\n",
    "def create_word_vectors(sentences):\n",
    "    '''\n",
    "    Input: List of sentences\n",
    "    Output: List of word vectors corresponding to each sentence, vocabulary\n",
    "    '''\n",
    "    sequence_length = max(len(x) for x in sentences)\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        new_sentence = sentence + [\"</s>\"] * num_padding\n",
    "        padded_sentences.append(new_sentence)\n",
    "    word_counts = Counter(itertools.chain(*padded_sentences))\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    word_vectors = np.array([[vocabulary[word] for word in sentence] for sentence in padded_sentences])\n",
    "  \n",
    "    return word_vectors, vocabulary, padded_sentences\n",
    "\n",
    "\n",
    "x, vocabulary,padded_sentences = create_word_vectors(sentences)\n",
    "\n",
    "\n",
    "def create_shuffle(x,y):\n",
    "    '''\n",
    "    Create an equal distribution of the positive and negative examples. \n",
    "    Please do not change this particular shuffling method.\n",
    "    '''\n",
    "    pos_len= len(pos_data)\n",
    "    neg_len= len(neg_data)\n",
    "    pos_len_train= int(0.8*pos_len)\n",
    "    neg_len_train= int(0.8*neg_len)\n",
    "    train_data= [(x[i],y[i]) for i in range(0, pos_len_train)]\n",
    "    train_data.extend([(x[i],y[i]) for i in range(pos_len, pos_len+ neg_len_train )])\n",
    "    test_data=[(x[i],y[i]) for i in range(pos_len_train, pos_len)]\n",
    "    test_data.extend([(x[i],y[i]) for i in range(pos_len+ neg_len_train, len(x) )])\n",
    "    \n",
    "    random.shuffle(train_data)\n",
    "    x_train=[i[0] for i in train_data]\n",
    "    y_train=[i[1] for i in train_data]\n",
    "    random.shuffle(test_data)\n",
    "    x_test=[i[0] for i in test_data]\n",
    "    y_test=[i[1] for i in test_data]\n",
    "    \n",
    "    x_train=np.array(x_train)\n",
    "    y_train=np.array(y_train)\n",
    "    x_test= np.array(x_test)\n",
    "    y_test= np.array(y_test)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_test, y_test= create_shuffle(x,y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('batch size', 20)\n",
      "('embedding dimensions', 200)\n",
      "('convolution filters', [2, 3, 4, 5])\n",
      "('dropout probability', 0.5)\n",
      "('optimizer', 'rmsprop')\n",
      "('maximum gradient', 5.0)\n",
      "('learning rate (step size)', 0.001)\n",
      "('epochs to train for', 20)\n",
      "Iter [0] Train: Time: 1.100s, Training Accuracy: 86.500             --- Dev Accuracy thus far: 87.333\n",
      "Iter [0] F1 score  for train  0.019 for DEV 0.098\n",
      "Iter [1] Train: Time: 1.034s, Training Accuracy: 89.500             --- Dev Accuracy thus far: 88.333\n",
      "Iter [1] F1 score  for train  0.316 for DEV 0.258\n",
      "Iter [2] Train: Time: 1.035s, Training Accuracy: 96.667             --- Dev Accuracy thus far: 88.667\n",
      "Iter [2] F1 score  for train  0.805 for DEV 0.290\n",
      "Iter [3] Train: Time: 1.047s, Training Accuracy: 99.500             --- Dev Accuracy thus far: 89.333\n",
      "Iter [3] F1 score  for train  0.916 for DEV 0.330\n",
      "Iter [4] Train: Time: 1.037s, Training Accuracy: 99.500             --- Dev Accuracy thus far: 89.333\n",
      "Iter [4] F1 score  for train  0.913 for DEV 0.307\n",
      "Iter [5] Train: Time: 1.050s, Training Accuracy: 99.583             --- Dev Accuracy thus far: 89.000\n",
      "Iter [5] F1 score  for train  0.915 for DEV 0.285\n",
      "Iter [6] Train: Time: 1.044s, Training Accuracy: 99.667             --- Dev Accuracy thus far: 89.000\n",
      "Iter [6] F1 score  for train  0.920 for DEV 0.285\n",
      "Iter [7] Train: Time: 1.045s, Training Accuracy: 99.667             --- Dev Accuracy thus far: 89.333\n",
      "Iter [7] F1 score  for train  0.920 for DEV 0.307\n",
      "Iter [8] Train: Time: 1.049s, Training Accuracy: 99.833             --- Dev Accuracy thus far: 89.333\n",
      "Iter [8] F1 score  for train  0.924 for DEV 0.307\n",
      "Saved checkpoint to ./cnn-0009.params\n",
      "Iter [9] Train: Time: 1.040s, Training Accuracy: 99.917             --- Dev Accuracy thus far: 89.333\n",
      "Iter [9] F1 score  for train  0.930 for DEV 0.307\n",
      "Iter [10] Train: Time: 1.029s, Training Accuracy: 100.000             --- Dev Accuracy thus far: 89.333\n",
      "Iter [10] F1 score  for train  0.933 for DEV 0.307\n",
      "Iter [11] Train: Time: 1.051s, Training Accuracy: 100.000             --- Dev Accuracy thus far: 89.333\n",
      "Iter [11] F1 score  for train  0.933 for DEV 0.307\n",
      "Iter [12] Train: Time: 1.044s, Training Accuracy: 100.000             --- Dev Accuracy thus far: 89.333\n",
      "Iter [12] F1 score  for train  0.933 for DEV 0.307\n",
      "Iter [13] Train: Time: 1.049s, Training Accuracy: 100.000             --- Dev Accuracy thus far: 89.333\n",
      "Iter [13] F1 score  for train  0.933 for DEV 0.307\n",
      "Iter [14] Train: Time: 1.048s, Training Accuracy: 100.000             --- Dev Accuracy thus far: 89.333\n",
      "Iter [14] F1 score  for train  0.933 for DEV 0.307\n",
      "Iter [15] Train: Time: 1.049s, Training Accuracy: 100.000             --- Dev Accuracy thus far: 89.333\n",
      "Iter [15] F1 score  for train  0.933 for DEV 0.307\n",
      "Iter [16] Train: Time: 1.047s, Training Accuracy: 100.000             --- Dev Accuracy thus far: 89.333\n",
      "Iter [16] F1 score  for train  0.933 for DEV 0.307\n",
      "Iter [17] Train: Time: 1.041s, Training Accuracy: 100.000             --- Dev Accuracy thus far: 89.333\n",
      "Iter [17] F1 score  for train  0.933 for DEV 0.307\n",
      "Iter [18] Train: Time: 1.048s, Training Accuracy: 100.000             --- Dev Accuracy thus far: 89.333\n",
      "Iter [18] F1 score  for train  0.933 for DEV 0.307\n",
      "Saved checkpoint to ./cnn-0019.params\n",
      "Iter [19] Train: Time: 1.053s, Training Accuracy: 100.000             --- Dev Accuracy thus far: 89.333\n",
      "Iter [19] F1 score  for train  0.933 for DEV 0.307\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "We now define the neural architecture of the CNN. The architecture is defined as : (10)\n",
    "\n",
    "1) Embedding layer that converts the vector representation of the sentence from a one-hot encoding to a fixed sized word embedding\n",
    "   (mx.sym.Embedding)\n",
    "   \n",
    "2) Convolution + activation + max pooling layer \n",
    "   (mx.sym.Convolution+ mx.sym.Activation+ mx.sym.Pooling)\n",
    "   This procedure is to be followed for different sizes of filters (the filters corresponding to size 2 looks at the bigram distribution, 3 looks at trigram etc. \n",
    "\n",
    "3) Concat all the filters together (mx.sym.Concat)\n",
    "\n",
    "4) Pass the results through a fully Connected layer of size 2 and then run softmax on it. \n",
    "   (mx.sym.FullyConnected, mx.sym.SoftmaxOutput)\n",
    "   \n",
    "\n",
    "We then initialize the intermediate layers of appropriate size and train the model using back prop. (10)\n",
    "(Look up the mxnet tutorial if you have any doubt)\n",
    "\n",
    "Run the classifier and for each epoch with a specified batch size observe the accuracy on the training set and test set (5)\n",
    "\n",
    "\n",
    "Default parameters:\n",
    "\n",
    "1) No of epochs = 10\n",
    "2) Batch size = 20\n",
    "3) Size of word embeddings = 200\n",
    "4) Size of filters =[2,3,4,5]\n",
    "5) Filter embedding= 100\n",
    "6) Optimizer = rmsprop\n",
    "7) learning rate = 0.005\n",
    "\n",
    "'''\n",
    "sentence_size = x_train.shape[1]\n",
    "vocab_size = len(vocabulary)\n",
    "batch_size = 20\n",
    "print('batch size', batch_size)\n",
    "\n",
    "input_x = mx.sym.Variable('data') \n",
    "input_y = mx.sym.Variable('softmax_label') \n",
    "\n",
    "num_embed = 200 \n",
    "print('embedding dimensions', num_embed)\n",
    "\n",
    "\n",
    "embed_layer = mx.sym.Embedding(data=input_x, input_dim=vocab_size, output_dim=num_embed, name='vocab_embed')\n",
    "\n",
    "conv_input = mx.sym.Reshape(data=embed_layer, shape=(batch_size, 1, sentence_size, num_embed))\n",
    "\n",
    "# create convolution + (max) pooling layer for each filter operation\n",
    "filter_list=[2, 3, 4, 5] # the size of filters to use\n",
    "print('convolution filters', filter_list)\n",
    "\n",
    "num_filter=100\n",
    "pooled_outputs = []\n",
    "for filter_size in filter_list:\n",
    "    convi = mx.sym.Convolution(data=conv_input, kernel=(filter_size, num_embed), num_filter=num_filter)\n",
    "    relui = mx.sym.Activation(data=convi, act_type='relu')\n",
    "    pooli = mx.sym.Pooling(data=relui, pool_type='max', kernel=(sentence_size - filter_size + 1, 1), stride=(1, 1))\n",
    "    pooled_outputs.append(pooli)\n",
    "\n",
    "# combine all pooled outputs\n",
    "total_filters = num_filter * len(filter_list)\n",
    "concat = mx.sym.Concat(*pooled_outputs, dim=1)\n",
    "\n",
    "# reshape for next layer\n",
    "h_pool = mx.sym.Reshape(data=concat, shape=(batch_size, total_filters))\n",
    "\n",
    "# dropout layer\n",
    "dropout = 0.5\n",
    "print('dropout probability', dropout)\n",
    "\n",
    "if dropout > 0.0:\n",
    "    h_drop = mx.sym.Dropout(data=h_pool, p=dropout)\n",
    "else:\n",
    "    h_drop = h_pool\n",
    "\n",
    "# fully connected layer\n",
    "num_label = 2\n",
    "\n",
    "cls_weight = mx.sym.Variable('cls_weight')\n",
    "cls_bias = mx.sym.Variable('cls_bias')\n",
    "\n",
    "fc = mx.sym.FullyConnected(data=h_pool, weight=cls_weight, bias=cls_bias, num_hidden=num_label)\n",
    "\n",
    "# softmax output\n",
    "sm = mx.sym.SoftmaxOutput(data=fc, label=input_y, name='softmax')\n",
    "\n",
    "# set CNN pointer to the \"back\" of the network\n",
    "cnn = sm\n",
    "\n",
    "from collections import namedtuple\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Define the structure of our CNN Model (as a named tuple)\n",
    "CNNModel = namedtuple(\"CNNModel\", ['cnn_exec', 'symbol', 'data', 'label', 'param_blocks'])\n",
    "\n",
    "# Define what device to train/test on, use GPU if available\n",
    "ctx = mx.gpu() if mx.test_utils.list_gpus() else mx.cpu()\n",
    "\n",
    "arg_names = cnn.list_arguments()\n",
    "\n",
    "input_shapes = {}\n",
    "input_shapes['data'] = (batch_size, sentence_size)\n",
    "\n",
    "arg_shape, out_shape, aux_shape = cnn.infer_shape(**input_shapes)\n",
    "arg_arrays = [mx.nd.zeros(s, ctx) for s in arg_shape]\n",
    "args_grad = {}\n",
    "for shape, name in zip(arg_shape, arg_names):\n",
    "    if name in ['softmax_label', 'data']: # input, output\n",
    "        continue\n",
    "    args_grad[name] = mx.nd.zeros(shape, ctx)\n",
    "\n",
    "cnn_exec = cnn.bind(ctx=ctx, args=arg_arrays, args_grad=args_grad, grad_req='add')\n",
    "\n",
    "param_blocks = []\n",
    "arg_dict = dict(zip(arg_names, cnn_exec.arg_arrays))\n",
    "initializer = mx.initializer.Uniform(0.1)\n",
    "for i, name in enumerate(arg_names):\n",
    "    if name in ['softmax_label', 'data']: # input, output\n",
    "        continue\n",
    "    initializer(mx.init.InitDesc(name), arg_dict[name])\n",
    "\n",
    "    param_blocks.append( (i, arg_dict[name], args_grad[name], name) )\n",
    "\n",
    "data = cnn_exec.arg_dict['data']\n",
    "label = cnn_exec.arg_dict['softmax_label']\n",
    "\n",
    "cnn_model= CNNModel(cnn_exec=cnn_exec, symbol=cnn, data=data, label=label, param_blocks=param_blocks)\n",
    "\n",
    "'''\n",
    "Train the cnn_model using back prop\n",
    "'''\n",
    "\n",
    "optimizer = 'rmsprop'\n",
    "max_grad_norm = 5.0\n",
    "learning_rate = 0.001\n",
    "epoch = 20\n",
    "\n",
    "print('optimizer', optimizer)\n",
    "print('maximum gradient', max_grad_norm)\n",
    "print('learning rate (step size)', learning_rate)\n",
    "print('epochs to train for', epoch)\n",
    "\n",
    "# create optimizer\n",
    "opt = mx.optimizer.create(optimizer)\n",
    "opt.lr = learning_rate\n",
    "\n",
    "updater = mx.optimizer.get_updater(opt)\n",
    "\n",
    "# For each training epoch\n",
    "for iteration in range(epoch):\n",
    "    tic = time.time()\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    c=0\n",
    "    F1_train=0\n",
    "    # Over each batch of training data\n",
    "    for begin in range(0, x_train.shape[0], batch_size):\n",
    "        batchX = x_train[begin:begin+batch_size]\n",
    "        batchY = y_train[begin:begin+batch_size]\n",
    "        if batchX.shape[0] != batch_size:\n",
    "            continue\n",
    "\n",
    "        cnn_model.data[:] = batchX\n",
    "        cnn_model.label[:] = batchY\n",
    "\n",
    "        # forward\n",
    "        cnn_model.cnn_exec.forward(is_train=True)\n",
    "\n",
    "        # backward\n",
    "        cnn_model.cnn_exec.backward()\n",
    "\n",
    "        # eval on training data\n",
    "        z1=np.argmax(cnn_model.cnn_exec.outputs[0].asnumpy(), axis=1)\n",
    "        num_correct += sum(batchY == z1)\n",
    "        F1_train+=f1_score(z1, batchY, average='binary')\n",
    "        c+=1\n",
    "        num_total += len(batchY)\n",
    "       \n",
    "        # update weights\n",
    "        norm = 0\n",
    "        for idx, weight, grad, name in cnn_model.param_blocks:\n",
    "            grad /= batch_size\n",
    "            l2_norm = mx.nd.norm(grad).asscalar()\n",
    "            norm += l2_norm * l2_norm\n",
    "\n",
    "        norm = math.sqrt(norm)\n",
    "        for idx, weight, grad, name in cnn_model.param_blocks:\n",
    "            if norm > max_grad_norm:\n",
    "                grad *= (max_grad_norm / norm)\n",
    "\n",
    "            updater(idx, grad, weight)\n",
    "\n",
    "            # reset gradient to zero\n",
    "            grad[:] = 0.0\n",
    "    F1_train/=c\n",
    "    # End of training loop for this epoch\n",
    "    toc = time.time()\n",
    "    train_time = toc - tic\n",
    "    train_acc = num_correct * 100 / float(num_total)\n",
    "\n",
    "    # Saving checkpoint to disk\n",
    "    if (iteration + 1) % 10 == 0:\n",
    "        prefix = 'cnn'\n",
    "        cnn_model.symbol.save('./%s-symbol.json' % prefix)\n",
    "        save_dict = {('arg:%s' % k) : v  for k, v in cnn_model.cnn_exec.arg_dict.items()}\n",
    "        save_dict.update({('aux:%s' % k) : v for k, v in cnn_model.cnn_exec.aux_dict.items()})\n",
    "        param_name = './%s-%04d.params' % (prefix, iteration)\n",
    "        mx.nd.save(param_name, save_dict)\n",
    "        print('Saved checkpoint to %s' % param_name)\n",
    "\n",
    "\n",
    "    # Evaluate model after this epoch on dev (test) set\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    c=0\n",
    "    F1=0\n",
    "    # For each test batch\n",
    "    for begin in range(0, x_test.shape[0], batch_size):\n",
    "        batchX = x_test[begin:begin+batch_size]\n",
    "        batchY = y_test[begin:begin+batch_size]\n",
    "\n",
    "        if batchX.shape[0] != batch_size:\n",
    "            continue\n",
    "\n",
    "        cnn_model.data[:] = batchX\n",
    "        cnn_model.cnn_exec.forward(is_train=False)\n",
    "        z=np.argmax(cnn_model.cnn_exec.outputs[0].asnumpy(), axis=1)\n",
    "        num_correct += sum(batchY == z)\n",
    "        num_total += len(batchY)\n",
    "        F1+=f1_score(z, batchY, average='binary')\n",
    "        c+=1\n",
    "    dev_acc = num_correct * 100 / float(num_total)\n",
    "\n",
    "    F1/=c\n",
    "    print('Iter [%d] Train: Time: %.3fs, Training Accuracy: %.3f \\\n",
    "            --- Dev Accuracy thus far: %.3f' % (iteration, train_time, train_acc, dev_acc))\n",
    "    print(\"Iter [%d] F1 score  for train  %.3f for DEV %.3f\" %(iteration,F1_train,F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSo far, the assignment has been posed in a manner so that you can refer to directly the mxnet tutorial on the same problem. \\n\\nThe final 15 marks is meant to carry out experimentations of your own and observe how the results change by experimentation. \\n\\n1) Would the results improve if instead of using the word embeddings that is based solely on frequency, if you have been able to incorporate sub-word information\\n   (In short run fasttext on the corpus and use the word embeddings generated by fastetxt). (8)\\n   \\n2) Accuracy might not be the best way to measure the performance of a skewed dataset. What other metrics would you use ? Why? \\n   Experiment with different hyper-paramters to show the performance in terms of metric? \\n   You can assume that we want to identify all the medically relevant tweets (i.e. tweets with 'yes' class more). (7)\\n    \\n\\nDelivearbles:\\n\\nThe ipython notebook with the results to each part of the question. \\n\\n\\nP.S: This assignment is part of a research question I am working on my free time. So if you have any insights, I'd love to hear them. \\nHappy coding \\n\\nRitam Dutt\\n14CS30041\\n\\n\""
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "So far, the assignment has been posed in a manner so that you can refer to directly the mxnet tutorial on the same problem. \n",
    "\n",
    "The final 15 marks is meant to carry out experimentations of your own and observe how the results change by experimentation. \n",
    "\n",
    "1) Would the results improve if instead of using the word embeddings that is based solely on frequency, if you have been able to incorporate sub-word information\n",
    "   (In short run fasttext on the corpus and use the word embeddings generated by fastetxt). (8)\n",
    "   \n",
    "2) Accuracy might not be the best way to measure the performance of a skewed dataset. What other metrics would you use ? Why? \n",
    "   Experiment with different hyper-paramters to show the performance in terms of metric? \n",
    "   You can assume that we want to identify all the medically relevant tweets (i.e. tweets with 'yes' class more). (7)\n",
    "    \n",
    "\n",
    "Delivearbles:\n",
    "\n",
    "The ipython notebook with the results to each part of the question. \n",
    "\n",
    "\n",
    "P.S: This assignment is part of a research question I am working on my free time. So if you have any insights, I'd love to hear them. \n",
    "Happy coding \n",
    "\n",
    "Ritam Dutt\n",
    "14CS30041\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "def create_ft_weights(sentences, num_embed, vocabulary, epoch):\n",
    "    model = FastText(size=num_embed, window=3, min_count=1)  # instantiate\n",
    "    model.build_vocab(sentences=sentences)\n",
    "    model.train(sentences=sentences, total_examples=model.corpus_count, epochs=epoch)  # train\n",
    "    weights = []\n",
    "    for word in vocabulary:\n",
    "        weights.append(model.wv[word])\n",
    "    return np.asarray(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import f1_score\n",
    "def train_and_test(num_embed, filter_embed, batch_size, vocab_size, sentence_size, filter_list, optimizer, learning_rate, epoch,weight_init, ft = True,f1score = True, verbose = True):\n",
    "    input_x = mx.sym.Variable('data') # placeholder for input data\n",
    "    input_y = mx.sym.Variable('softmax_label') # placeholder for output label\n",
    "\n",
    "    if ft == True:\n",
    "        weight = mx.sym.Variable('vocab_embed_weight')\n",
    "    '''\n",
    "    Define the first network layer (embedding)\n",
    "    '''\n",
    "\n",
    "    # create embedding layer to learn representation of words in a lower dimensional subspace (much like word2vec)\n",
    "\n",
    "    #print('embedding dimensions', num_embed)\n",
    "    if ft == False:\n",
    "        embed_layer = mx.sym.Embedding(data=input_x, input_dim=vocab_size, output_dim=num_embed, name='vocab_embed')\n",
    "    else:\n",
    "        embed_layer = mx.sym.Embedding(data=input_x, input_dim=vocab_size, output_dim=num_embed, weight = weight, name='vocab_embed')\n",
    "\n",
    "    # reshape embedded data for next layer\n",
    "    conv_input = mx.sym.Reshape(data=embed_layer, shape=(batch_size, 1, sentence_size, num_embed))\n",
    "\n",
    "    # create convolution + (max) pooling layer for each filter operation\n",
    "     # the size of filters to use\n",
    "    #print('convolution filters', filter_list)\n",
    "\n",
    "    num_filter=1\n",
    "    pooled_outputs = []\n",
    "    for filter_size in filter_list:\n",
    "        convi = mx.sym.Convolution(data=conv_input, kernel=(filter_size, filter_embed), num_filter=num_filter)\n",
    "        relui = mx.sym.Activation(data=convi, act_type='relu')\n",
    "        pooli = mx.sym.Pooling(data=relui, pool_type='max', kernel=(sentence_size - filter_size + 1, 1), stride=(1, 1))\n",
    "        pooled_outputs.append(pooli)\n",
    "\n",
    "    # combine all pooled outputs\n",
    "    total_filters = num_filter * len(filter_list)\n",
    "    concat = mx.sym.Concat(*pooled_outputs, dim=1)\n",
    "\n",
    "    # reshape for next layer\n",
    "    h_pool = mx.sym.Reshape(data=concat, shape=(batch_size, total_filters*(num_embed - filter_embed + 1)))\n",
    "\n",
    "    num_label = 2\n",
    "\n",
    "    cls_weight = mx.sym.Variable('cls_weight')\n",
    "    cls_bias = mx.sym.Variable('cls_bias')\n",
    "\n",
    "    fc = mx.sym.FullyConnected(data=h_pool, weight=cls_weight, bias=cls_bias, num_hidden=num_label)\n",
    "\n",
    "    # softmax output\n",
    "    sm = mx.sym.SoftmaxOutput(data=fc, label=input_y, name='softmax')\n",
    "\n",
    "    # set CNN pointer to the \"back\" of the network\n",
    "    cnn = sm\n",
    "\n",
    "    # Define what device to train/test on, use GPU if available\n",
    "    ctx = mx.gpu() if mx.test_utils.list_gpus() else mx.cpu()\n",
    "\n",
    "    arg_names = cnn.list_arguments()\n",
    "#     print(arg_names)\n",
    "#     exit()\n",
    "    input_shapes = {}\n",
    "    input_shapes['data'] = (batch_size, sentence_size)\n",
    "\n",
    "    arg_shape, out_shape, aux_shape = cnn.infer_shape(**input_shapes)\n",
    "    arg_arrays = [mx.nd.zeros(s, ctx) for s in arg_shape]\n",
    "    args_grad = {}\n",
    "    for shape, name in zip(arg_shape, arg_names):\n",
    "        if name in ['softmax_label', 'data']: # input, output\n",
    "            continue\n",
    "        args_grad[name] = mx.nd.zeros(shape, ctx)\n",
    "\n",
    "    cnn_exec = cnn.bind(ctx=ctx, args=arg_arrays, args_grad=args_grad, grad_req='add')\n",
    "\n",
    "    param_blocks = []\n",
    "    arg_dict = dict(zip(arg_names, cnn_exec.arg_arrays))\n",
    "    initializer = mx.initializer.Uniform(0.1)\n",
    "    for i, name in enumerate(arg_names):\n",
    "        if name in ['softmax_label', 'data']: # input, output\n",
    "            continue\n",
    "        if ft == True:\n",
    "            if name == 'vocab_embed_weight':\n",
    "                initializer_2 = mx.initializer.Load({name:weight_init})\n",
    "                initializer_2(mx.init.InitDesc(name), arg_dict[name])\n",
    "                print('FastText weights initialized!')\n",
    "            else:\n",
    "                initializer(mx.init.InitDesc(name), arg_dict[name])\n",
    "        else:\n",
    "            initializer(mx.init.InitDesc(name), arg_dict[name])\n",
    "\n",
    "        param_blocks.append( (i, arg_dict[name], args_grad[name], name) )\n",
    "\n",
    "    data = cnn_exec.arg_dict['data']\n",
    "    label = cnn_exec.arg_dict['softmax_label']\n",
    "\n",
    "    '''\n",
    "    Train the cnn_model using back prop\n",
    "    '''\n",
    "\n",
    "    # create optimizer\n",
    "    opt = mx.optimizer.create(optimizer)\n",
    "    opt.lr = learning_rate\n",
    "\n",
    "    updater = mx.optimizer.get_updater(opt)\n",
    "\n",
    "    # For each training epoch\n",
    "    max_test_acc = 0\n",
    "    for iteration in range(epoch):\n",
    "        #tic = time.time()\n",
    "        score = []\n",
    "        k_labels = []\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        # Over each batch of training data\n",
    "        for begin in range(0, x_train.shape[0], batch_size):\n",
    "            batchX = x_train[begin:begin+batch_size]\n",
    "            batchY = y_train[begin:begin+batch_size]\n",
    "            if batchX.shape[0] != batch_size:\n",
    "                continue\n",
    "\n",
    "            data[:] = batchX\n",
    "            label[:] = batchY\n",
    "\n",
    "            # forward\n",
    "            cnn_exec.forward(is_train=True)\n",
    "\n",
    "            # backward\n",
    "            cnn_exec.backward()\n",
    "\n",
    "            # eval on training data\n",
    "            num_correct += sum(batchY == np.argmax(cnn_exec.outputs[0].asnumpy(), axis=1))\n",
    "            \n",
    "            num_total += len(batchY)\n",
    "            score.extend(np.argmax(cnn_exec.outputs[0].asnumpy(), axis=1).tolist())\n",
    "            k_labels.extend(batchY.tolist())\n",
    "\n",
    "\n",
    "            # update weights\n",
    "            norm = 0\n",
    "            for idx, weight, grad, name in param_blocks:\n",
    "                grad /= batch_size\n",
    "                l2_norm = mx.nd.norm(grad).asscalar()\n",
    "                norm += l2_norm * l2_norm\n",
    "\n",
    "            norm = np.sqrt(norm)\n",
    "            for idx, weight, grad, name in param_blocks:\n",
    "    #             if norm > max_grad_norm:\n",
    "    #                 grad *= (max_grad_norm / norm)\n",
    "\n",
    "                updater(idx, grad, weight)\n",
    "\n",
    "                # reset gradient to zero\n",
    "                grad[:] = 0.0\n",
    "\n",
    "        # End of training loop for this epoch\n",
    "    #     toc = time.time()\n",
    "    #     train_time = toc - tic\n",
    "        train_acc = num_correct * 100 / float(num_total)\n",
    "        \n",
    "        train_f1 = f1_score(score,k_labels)\n",
    "        # Saving checkpoint to disk\n",
    "    #     if (iteration + 1) % 10 == 0:\n",
    "    #         prefix = 'cnn'\n",
    "    #         symbol.save('./%s-symbol.json' % prefix)\n",
    "    #         save_dict = {('arg:%s' % k) : v  for k, v in cnn_exec.arg_dict.items()}\n",
    "    #         save_dict.update({('aux:%s' % k) : v for k, v in cnn_exec.aux_dict.items()})\n",
    "    #         param_name = './%s-%04d.params' % (prefix, iteration)\n",
    "    #         mx.nd.save(param_name, save_dict)\n",
    "    #         print('Saved checkpoint to %s' % param_name)\n",
    "\n",
    "\n",
    "        # Evaluate model after this epoch on dev (test) set\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        test_score = []\n",
    "        test_label = []\n",
    "\n",
    "\n",
    "        # For each test batch\n",
    "        for begin in range(0, x_test.shape[0], batch_size):\n",
    "            batchX = x_test[begin:begin+batch_size]\n",
    "            batchY = y_test[begin:begin+batch_size]\n",
    "\n",
    "            if batchX.shape[0] != batch_size:\n",
    "                continue\n",
    "\n",
    "            data[:] = batchX\n",
    "            cnn_exec.forward(is_train=False)\n",
    "\n",
    "            num_correct += sum(batchY == np.argmax(cnn_exec.outputs[0].asnumpy(), axis=1))\n",
    "            test_score.extend(np.argmax(cnn_exec.outputs[0].asnumpy(), axis=1).tolist())\n",
    "            test_label.extend(batchY.tolist())\n",
    "            num_total += len(batchY)\n",
    "\n",
    "        dev_acc = num_correct * 100 / float(num_total)\n",
    "        test_f1 = f1_score(test_score,test_label)\n",
    "        if dev_acc > max_test_acc: \n",
    "            max_test_acc = dev_acc\n",
    "        if verbose == True:\n",
    "            print('Epoch [%d] Train: Training F1 score: %.3f \\\n",
    "                        --- Test F1 score thus far: %.3f' % (iteration, train_f1, test_f1))\n",
    "\n",
    "    return(test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = create_ft_weights(padded_sentences , num_embed, vocabulary, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText weights initialized!\n",
      "Epoch [0] Train: Training F1 score: 0.135                         --- Test F1 score thus far: 0.174\n",
      "Epoch [1] Train: Training F1 score: 0.472                         --- Test F1 score thus far: 0.393\n",
      "Epoch [2] Train: Training F1 score: 0.884                         --- Test F1 score thus far: 0.441\n",
      "Epoch [3] Train: Training F1 score: 0.970                         --- Test F1 score thus far: 0.377\n",
      "Epoch [4] Train: Training F1 score: 0.982                         --- Test F1 score thus far: 0.386\n",
      "Epoch [5] Train: Training F1 score: 0.988                         --- Test F1 score thus far: 0.400\n",
      "Epoch [6] Train: Training F1 score: 1.000                         --- Test F1 score thus far: 0.400\n",
      "Epoch [7] Train: Training F1 score: 1.000                         --- Test F1 score thus far: 0.400\n",
      "Epoch [8] Train: Training F1 score: 1.000                         --- Test F1 score thus far: 0.393\n",
      "Epoch [9] Train: Training F1 score: 1.000                         --- Test F1 score thus far: 0.400\n",
      "Epoch [10] Train: Training F1 score: 1.000                         --- Test F1 score thus far: 0.400\n",
      "Epoch [11] Train: Training F1 score: 1.000                         --- Test F1 score thus far: 0.400\n",
      "Epoch [12] Train: Training F1 score: 1.000                         --- Test F1 score thus far: 0.400\n",
      "Epoch [13] Train: Training F1 score: 1.000                         --- Test F1 score thus far: 0.400\n",
      "Epoch [14] Train: Training F1 score: 1.000                         --- Test F1 score thus far: 0.400\n",
      "Epoch [15] Train: Training F1 score: 1.000                         --- Test F1 score thus far: 0.400\n",
      "Epoch [16] Train: Training F1 score: 1.000                         --- Test F1 score thus far: 0.400\n",
      "Epoch [17] Train: Training F1 score: 1.000                         --- Test F1 score thus far: 0.400\n"
     ]
    }
   ],
   "source": [
    "filter_list =[2,3,4,5]\n",
    "epochs = 10\n",
    "batch_size = 20\n",
    "word_embeddings = 200\n",
    "#filter_list =[2,3,4,5]\n",
    "filter_embedding= 100\n",
    "optimizer = 'rmsprop'\n",
    "learning_rate = 0.005\n",
    "filter_embed = 100\n",
    "scores = train_and_test(num_embed, filter_embed, batch_size, vocab_size, sentence_size, filter_list, optimizer, learning_rate, epoch, matrix)\n",
    "#input_x = mx.sym.Variable('data') \n",
    "# placeholder for input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
